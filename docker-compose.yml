version: "3.8"

services:
  localinference:
    build: .
    container_name: localinference
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
    environment:
      - MODEL_DIR=/app/models
      - NVIDIA_VISIBLE_DEVICES=all
    ipc: host
    privileged: true  # Be careful with this in production
